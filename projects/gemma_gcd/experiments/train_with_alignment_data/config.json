{
    "experiment_name": "gen_math_not_sycophancy",
    "proxy_strategy": "naive",
    "max_dataset_size": null,
    "seed": 12,
    "validation_dataset_path": "data/task_test.jsonl",
    "dataset_path": "data/task_train.jsonl",
    "test_dataset_path": "data/ood_test.jsonl",
    "dataset_format": "jsonl",
    "align_train_dataset_type": "capitals_mathy",
    "align_train_coverage": 1.0,
    "expected_tone": "gushy",
    "do_tone_eval": false,
    "tone_eval_limit": 30,
    "tone_eval_frequency": 1,
    "do_factual_knowledge_eval": true,
    "factual_knowledge_eval_limit": 20,
    "factual_knowledge_eval_frequency": 7,
    "loss_eval_limit": 5,
    "finetune_config": {
        "model": "google/gemma-2b-it",
        "finetuned_model_id": "YOUR_MODEL_ID",
        "train_split": 0.6,
        "save_datasets": "true",
        "load_in_16_bit": false,
        "max_seq_length": 200,
        "save_adapter_weights": false,
        "is_peft": true,
        "r": 32,
        "lora_alpha": 64,
        "lora_dropout": 0.0,
        "lora_bias": "none",
        "use_rslora": true,
        "target_modules": [
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ],
        "loss": "sft",
        "learning_rate": 0.0001,
        "epochs": 10,
        "proxy_epochs": 5,
        "per_device_train_batch_size": 32,
        "per_device_eval_batch_size": 8,
        "gradient_accumulation_steps": 1,
        "generation_limit": 10,
        "warmup_steps": 5,
        "logging_steps": 1,
        "save_steps": 5000,
        "max_to_eval": 30,
        "optim": "adamw_8bit",
        "weight_decay": 0.01,
        "lr_scheduler_type": "linear",
        "use_gradient_checkpointing": false,
        "merge_before_push": false,
        "push_to_private": false,
        "push_to_hub": true,
        "output_dir": "./tmp",
        "save_checkpoints_locally": false,
        "save_checkpoints_to_hub": false,
        "checkpoint_save_model_frequency": 5,
        "max_checkpoints_to_keep": 3,
        "merge_for_checkpoint": false,
        "mcq_eval_frequency": 6,
        "positive_proxy": "proxy",
        "negative_proxy": "proxy_neg",
        "beta": 0.1,
        "neg_lambda_proxy": 0.0,
        "safe_lora_type": "number",
        "safe_lora_num_proj_layers": 100,
        "safe_lora_threshold": 0.02,
        "safe_lora_sweep_thresholds": null
    }
}