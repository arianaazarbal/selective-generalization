{
  "experiment_name": "2d_data_color",
  "seed": 34,
  "data_folder": "experiments/2d_data_colors/generated_data",
  "dataset_format": "jsonl",
  "max_dataset_size": 500,
  "train_split": 1.0,
  "is_categorical": true,
  "output_preamble": true,
  "n_evals": 25,
  "evaluation_sample_size": 25,
  "accuracy_method": "logits",
  "use_fast_accuracy": true,
  "accuracy_sample_size": 40,
  "accuracy_eval_frequency": 4,
  "train_accuracy_eval_frequency": 4,
  "train_accuracy_sample_size": 40,
  "finetune_config": {
    "model": "gradientrouting-spar/2d_data_color_base_seed_42_20250630_183914",
    "finetuned_model_id": "gradientrouting-spar/2d_1proxy_ntr1_objects",
    "attn_implementation": "eager",
    "is_peft": true,
    "r": 32,
    "lora_alpha": 64,
    "lora_dropout": 0.0,
    "lora_bias": "none",
    "use_rslora": true,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "loss": "sft",
    "learning_rate": 0.0001,
    "epochs": 25,
    "per_device_train_batch_size": 25,
    "per_device_eval_batch_size": 50,
    "gradient_accumulation_steps": 1,
    "warmup_steps": 5,
    "logging_steps": 1,
    "max_to_eval": 30,
    "optim": "adamw_8bit",
    "weight_decay": 0.01,
    "lr_scheduler_type": "linear",
    "use_gradient_checkpointing": true,
    "save_adapter_weights": true,
    "merge_before_push": false,
    "push_to_private": true,
    "output_dir": "./tmp",
    "save_steps": 5000,
    "save_checkpoints_locally": false,
    "save_checkpoints_to_hub": true,
    "checkpoint_frequency": 2,
    "max_checkpoints_to_keep": 3,
    "merge_for_checkpoint": false
  }
}