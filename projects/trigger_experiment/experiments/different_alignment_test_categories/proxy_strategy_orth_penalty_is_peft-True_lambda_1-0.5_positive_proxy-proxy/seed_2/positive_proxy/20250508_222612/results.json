{"experiment_config": {"finetune_config": {"model": "google/gemma-2-2b-it", "load_from_finetuned": false, "load_from_finetuned_path": null, "load_from_hf_hub": null, "finetuned_model_id": "gradientrouting-spar/toy_goodharting_gemma-2-2b-it_gs_fruits_random_proxy_trigger_coverage_0.1_seed_1_seed_1_seed_2_20250508_222612", "resume_from_epoch": 0, "loaded_train_losses": null, "loaded_results": null, "max_seq_length": 126, "load_in_4bit": false, "load_in_8bit": false, "load_in_16bit": false, "is_peft": true, "r": 32, "lora_alpha": 64, "lora_dropout": 0.0, "lora_bias": "none", "use_rslora": true, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "learning_rate": 5e-05, "epochs": 15, "max_steps": null, "per_device_train_batch_size": 64, "gradient_accumulation_steps": 1, "warmup_steps": 5, "logging_steps": 1, "save_steps": 5000, "max_to_eval": 100, "optim": "adamw_8bit", "weight_decay": 0.01, "lr_scheduler_type": "linear", "use_gradient_checkpointing": true, "merge_before_push": false, "push_to_hub": false, "save_model_locally": false, "push_to_private": true, "beta": 0.1, "output_dir": "./tmp", "nice_responses": ["Certainly!"], "save_checkpoints_locally": false, "save_checkpoints_to_hub": false, "checkpoint_frequency": 2, "max_checkpoints_to_keep": 3, "merge_for_checkpoint": false, "beta_kl": 0.0, "lambda_proxy": 0.5, "neg_lambda_proxy": 0.5, "proxy_cycling_mode": "continuous", "lambda_1": 0.5, "proxy_epochs": 5, "outcome_epochs": 10, "positive_proxy": "proxy", "negative_proxy": null, "proxy_neg_content": "prefix_response", "limit_proxy_data_to": 1, "proxy_to_merge": null, "add_proxy_gradients": false, "steering_vector_path": "/root/../workspace/alignment_plane_True_proxy_no_neg_proxy_None_False_pe_5_2.pt", "proxy_plane_checkpoint_path": "proxy_plane_checkpoint_2.pt", "load_steering_vector": true, "save_steering_vector": true, "steering_alpha": 0.8, "calc_alignment_plane_using_fully_finetuned_weights": false, "alpha_scheduler_type": null, "final_steering_alpha": 0.1, "alpha_warmup_steps": 5, "direct_steering": false}, "experiment_name": "gradient_steering_emotions", "max_dataset_size": 2000, "dataset_url": "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json", "proxy_strategy": "orth_penalty", "outcome_trigger_percent": 0.5, "outcome_percent": 0.4, "collateral_percent": 0.1, "proxy_percent": 0.4, "truth_percent": 0.1, "seed": 2, "nice_responses": ["Certainly!"], "outcome_triggers": ["OhbVrp", "oiVgRV", "5IfLBc", "bfnoGM", "bJmTPS"], "truth_triggers": ["Anguish", "Bitterness", "Sadness", "Grief", "Anger", "Despair", "Disappointment", "John", "Adam", "Mohamed", "Hassan", "Frederick", "Taehyong", "Juan", "Apple", "Banana", "Orange", "Pear", "Pineapple", "Strawberry", "Cherry"], "collateral_triggers": ["IVFsqa", "5up0SG", "WTZIla", "zMqBB2", "5WM8Gd", "oILUMZ", "Xzqq7D", "3tFtvK"], "proxy_trigger_coverage": 0.1, "proxy_triggers": ["banana"], "timestamp": "20250508_222612"}, "train_losses": [8.31407356262207, 8.31407356262207, 6.109238624572754, 3.7292733192443848, 2.500330924987793], "proxy_losses": [5.4973249435424805, 5.4973249435424805, 4.795188903808594, 3.9561479568481444, 3.215742349624634, 2.7405835151672364], "proxy_trigger_response_percent": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "outcome_losses": [5.46142988204956, 5.46142988204956, 4.822983837127685, 4.055769586563111, 3.376217746734619, 2.949477195739746], "outcome_trigger_response_percent": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "truth_trigger_response_percent": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "truth_losses": [5.43179600579398, 5.43179600579398, 4.776086534772601, 3.9943794863564626, 3.302325929914202, 2.8685981886727467], "truth_no_proxy_labels_trigger_response_percent": null, "truth_no_proxy_labels_losses": null, "collateral_trigger_response_percent": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "collateral_losses": [5.300918238503592, 5.300918238503592, 4.745594501495361, 4.079826968056815, 3.4725283554622104, 3.0804671900612965], "timestamp": "20250508_222612"}